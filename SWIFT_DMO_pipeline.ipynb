{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ec597a",
   "metadata": {},
   "source": [
    "# SWIFTCON 2025 - The \"fiducial\" simulation pipeline\n",
    "\n",
    "This notebook takes you through an example of running a dark-matter-only simulation and creating subhalo catalogues for subsequent analysis. The software used throughout this tutorial is the following:\n",
    "\n",
    "* **Initial conditions**: monofonIC. [Code repository](https://bitbucket.org/ohahn/monofonic/src/master/) and latest [reference paper](https://ui.adsabs.harvard.edu/abs/2020ascl.soft08024H/abstract).\n",
    "* **Cosmological integration**: SWIFT. [Code repository](https://gitlab.cosma.dur.ac.uk/swift/swiftsim) and [reference paper](https://ui.adsabs.harvard.edu/abs/2024MNRAS.530.2378S/abstract).\n",
    "* **Subhalo finding**: HBT-HERONS. [Code repository](https://github.com/SWIFTSIM/HBT-HERONS) and [reference paper](https://ui.adsabs.harvard.edu/abs/2025MNRAS.543.1339F/abstract).\n",
    "* **Subhalo property calculation**: SOAP. [Code repository](https://github.com/SWIFTSIM/SOAP) and [reference paper](https://ui.adsabs.harvard.edu/abs/2025JOSS...10.8252M/abstract).\n",
    "\n",
    "Created by Victor Forouhar Moreno (forouhar@strw.leidenuniv.nl) & Rob McGibbon (mcgibbon@strw.leidenuniv.nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f22971d-618a-4b6e-b835-0c1825f25dbb",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Several external libraries are required to compile and/or run the codes used in this tutorial, although there is considerable overlap across codes. If you are using the provided Docker image, all libraries are pre-installed and ready to go. If you are not using the provided image, you will need to install these libraries yourself.\n",
    "\n",
    "#### monofonIC\n",
    "- FFTW\n",
    "- GSL\n",
    "- HDF5\n",
    "\n",
    "#### SWIFT\n",
    "- HDF5 \n",
    "- MPI\n",
    "- FFTW\n",
    "- METIS\n",
    "- GSL\n",
    "\n",
    "#### HBT-HERONS\n",
    "- CMake\n",
    "- HDF5\n",
    "- MPI\n",
    "\n",
    "#### SOAP\n",
    "- mpi4py\n",
    "- h5py built with parallel HDF5\n",
    "- Standard python modules (see requirements.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f247d5c8-7db4-4867-9ed1-574af31faaeb",
   "metadata": {},
   "source": [
    "#### Setup on cosma\n",
    "\n",
    "This cell sets up dependencies for those running on cosma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc119c16-a007-4965-9d41-d7fd1ce46241",
   "metadata": {},
   "outputs": [],
   "source": [
    "module purge\n",
    "module load python/3.12.4 gnu_comp/14.1.0 openmpi/5.0.3 parallel_hdf5/1.12.3\n",
    "module load fftw gsl parmetis\n",
    "venv_name=\"$(echo $HOME | sed 's/home/apps/')/swift_workshop_env\"\n",
    "python=\"$venv_name/bin/python3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede3a9a6",
   "metadata": {},
   "source": [
    "# Generating initial conditions\n",
    "\n",
    "We need to download monofonIC first, for which we clone the official repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f361e6-9cc9-402b-9713-1b27ba1f6ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://bitbucket.org/ohahn/monofonic.git ./software/monofonic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95051db-a051-4cdf-8108-6e13c582808a",
   "metadata": {},
   "source": [
    "We then compile the code to generate the executable we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a17292-10b0-4ea7-8661-1f5696bf520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ./software/monofonic/\n",
    "mkdir build/ && cd build\n",
    "\n",
    "# Compilation options can be optionally specified at this stage.\n",
    "cmake ..\n",
    "make -j 4\n",
    "\n",
    "# Go back to original directory\n",
    "cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee3016-7489-4421-8294-cebf664abe46",
   "metadata": {},
   "source": [
    "### Running monofonIC\n",
    "\n",
    "The monofonIC executable takes the path to a parameter file as a runtime argument. In this parameter file, you can specify the redshift at which the ICs will be generated, the cosmological parameters and , among other things. \n",
    "\n",
    "We have provided a basic parameter file in `./parameter_files/monofonic/example.conf`. Note that the following mandatory parameters have been left unspecified, as we encourage you to play with their value:\n",
    "\n",
    "* `GridRes`: number of particles per dimension. Total number of particles will be the cube of this number.\n",
    "* `BoxLength`: Length of each side of the cubic box. **It should be in `Mpc/h`!** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb81b24-5661-4928-8027-3790ab52b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/jupyteruser/tutorial/software/monofonic/build/monofonIC ./parameter_files/monofonic/example.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4452982",
   "metadata": {},
   "source": [
    "# SWIFT - Running simulation\n",
    "\n",
    "[SWIFT](https://swift.strw.leidenuniv.nl/docs/index.html) is an open-source cosmological and astrophysical numerical solver designed to run efficiently on modern hardware. A comprehensive and extensive set of models for galaxy formation as well as planetary physics are provided alongside a large series of examples.\n",
    "\n",
    "#### Clone repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315a0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://gitlab.cosma.dur.ac.uk/swift/swiftsim.git\n",
    "cd swiftsim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a24db2",
   "metadata": {},
   "source": [
    "#### Configure and compile\n",
    "\n",
    "See the [notes here](https://swift.strw.leidenuniv.nl/docs/GettingStarted/compiling_code.html)\n",
    "\n",
    "Since HBTplus requires FOF information, we need to pass the `--enable-fof` flag\n",
    "\n",
    "Hydro considerations\n",
    "- Lots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e49ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "./autogen.sh\n",
    "./configure --enable-fof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0234421-d2f6-430b-a944-46cb1f859a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make -j 4\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de93f47a",
   "metadata": {},
   "source": [
    "#### Set parameters\n",
    "\n",
    "Define the units used by SWIFT, the cosmology of the run, and some parameters for running the simulation.\n",
    "\n",
    "Specify output location and frequency. Make sure to enable running the FOF before writing a snapshot.\n",
    "\n",
    "Information about how to run the FOF.\n",
    "\n",
    "Point the code to the initial conditions we generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a026579",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat > small_cosmo_volume_dm.yml << EOF\n",
    "# Define the system of units to use internally. \n",
    "InternalUnitSystem:\n",
    "  UnitMass_in_cgs:     1.98841e43    # 10^10 M_sun\n",
    "  UnitLength_in_cgs:   3.08567758e24 # 1 Mpc\n",
    "  UnitVelocity_in_cgs: 1e5           # 1 km/s\n",
    "  UnitCurrent_in_cgs:  1             # Amperes\n",
    "  UnitTemp_in_cgs:     1             # Kelvin\n",
    "\n",
    "Cosmology:\n",
    "  Omega_cdm:      0.3145\n",
    "  Omega_b:        0.0\n",
    "  Omega_lambda:   0.6855 \n",
    "  h:              0.673\n",
    "  a_begin:        0.02      \t     # z_ini = 50.\n",
    "  a_end:          1.0\t\t         # z_end = 0.\n",
    "\n",
    "# Parameters governing the time integration\n",
    "TimeIntegration:\n",
    "  dt_min:     1e-6 \n",
    "  dt_max:     1e-2 \n",
    "\n",
    "# Parameters for the self-gravity scheme\n",
    "Gravity:\n",
    "  eta:          0.025\n",
    "  MAC:          adaptive\n",
    "  theta_cr:     0.7\n",
    "  epsilon_fmm:  0.001\n",
    "  comoving_DM_softening:     0.0889     # 1/25th of the mean inter-particle separation: 88.9 kpc\n",
    "  max_physical_DM_softening: 0.0889     # 1/25th of the mean inter-particle separation: 88.9 kpc\n",
    "  mesh_side_length:       64\n",
    "\n",
    "# Parameters governing the snapshots\n",
    "Snapshots:\n",
    "  basename:            snap\n",
    "  delta_time:          1.15\n",
    "  scale_factor_first:  0.1     # z = 9\n",
    "  compression:         4\n",
    "  invoke_fof:          1\n",
    "\n",
    "# Parameters for running the friend-of-friends algorithm\n",
    "FOF:  \n",
    "  basename: fof_output                         # Filename for the FOF outputs.\n",
    "  min_group_size: 20                           # The minimum no. of particles required for a group.\n",
    "  linking_length_ratio: 0.2                    # Linking length in units of the mean inter-particle separation\n",
    "  seed_black_holes_enabled: 0\n",
    "  linking_types: [0, 1, 0, 0, 0, 0, 0]         # Use DM as the primary FOF linking type\n",
    "  attaching_types: [0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# Parameters governing the conserved quantities statistics\n",
    "Statistics:\n",
    "  delta_time:          1.01\n",
    "  scale_factor_first:  0.03\n",
    "\n",
    "# Parameters governing the cell tree\n",
    "Scheduler:\n",
    "  max_top_level_cells: 8\n",
    "  cell_split_size:     50\n",
    "  \n",
    "# Parameters related to the initial conditions\n",
    "InitialConditions:\n",
    "  file_name:                   small_cosmo_volume.hdf5\n",
    "  periodic:                    1\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8251d1",
   "metadata": {},
   "source": [
    "#### Run the code\n",
    "\n",
    "Here we are running the non-MPI version of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a8d763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "./swiftsim/swift --cosmology --self-gravity --fof --threads=4 small_cosmo_volume_dm.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e50eb",
   "metadata": {},
   "source": [
    "#### Output of run\n",
    "\n",
    "- `snap_xxxx.hdf5` - Particles and their properties\n",
    "- `fof_output_xxxx.hdf5` - FOF halo catalogue (FOF IDs stored in snapshots)\n",
    "- `statistics.txt` - Global properties of the simulation over time\n",
    "- `timesteps.txt` - What the simulation did during it's timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e5c5c-9e50-4fe3-a3a3-efeedce3b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc4c24",
   "metadata": {},
   "source": [
    "# HBTplus - Halo finding\n",
    "\n",
    "HBT uses the FOF catalogues from SWIFT to as candidates for central subhalos. Once a subhalo has been identified at any snapshot, the particles remain associated to it even if it falls into a larger halo. Using this information it is possible to identify satellite subhalos.\n",
    "\n",
    "This method requires that HBTplus is run on a range of snapshots from the simulation, it cannot be run on a single snapshot. We recommend at least ~64 snapshot spaced evenly in $\\log a$ (we did less in this notebook since it's just an example).\n",
    "\n",
    "Hydro considerations\n",
    "- SWIFT particle splitting\n",
    "- Which particles to use as tracers\n",
    "- Number of tracers for an object to be resolved\n",
    "- Thermal energy of gas particles\n",
    "\n",
    "#### Clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfc2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/SWIFTSIM/HBTplus.git\n",
    "cd HBTplus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac609b",
   "metadata": {},
   "source": [
    "#### Configure and compile\n",
    "\n",
    "Use `ccmake` to see what options are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmake -B$PWD/build -D HBT_USE_OPENMP=ON -D HBT_DM_ONLY=ON -D HBT_UNSIGNED_LONG_ID_OUTPUT=OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5d10d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd build\n",
    "make -j 4\n",
    "cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3f033",
   "metadata": {},
   "source": [
    "#### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat > HBT_config.txt << EOF\n",
    "# Configuration file used to run on small DMO simulation \n",
    "\n",
    "# Compulsary Params\n",
    "SnapshotPath ./                      # Location of snapshots\n",
    "SnapshotFileBase snap                # Basename of snapshot files\n",
    "HaloPath ./                          # Location of FOF ID files (normal snapshot for SWIFT)\n",
    "SubhaloPath HBT_output               # Output directory\n",
    "ParticlesSplit 0                     # Do we have gas particles that split\n",
    "MergeTrappedSubhalos 1               # Allow subhalos to merge\n",
    "# For the swiftsim reader these will be read in from snapshots automatically\n",
    "BoxSize -1\n",
    "SofteningHalo -1\n",
    "MaxPhysicalSofteningHalo -1\n",
    "\n",
    "# Reader\n",
    "SnapshotFormat swiftsim\n",
    "GroupFileFormat swiftsim_particle_index\n",
    "MinSnapshotIndex 0                  # First snapshot to use\n",
    "MaxSnapshotIndex 18                 # Final snapshot to use\n",
    "MaxConcurrentIO 8                   # Number of cores for IO\n",
    "MinNumPartOfSub 20                  # Minimum number of particles in a subhalo\n",
    "\n",
    "# Units\n",
    "MassInMsunh 6.81e9                  # Removes h factors from the final output \n",
    "LengthInMpch 0.681                  # Removes h factors from the final output\n",
    "VelInKmS 1\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac291daa",
   "metadata": {},
   "source": [
    "##### Run\n",
    "\n",
    "Run with a single rank as we will use 8 threads\n",
    "\n",
    "For FLAMINGO we were often IO limited, but this is not the case for COLIBRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da1acae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "export OMP_NUM_THREADS=8\n",
    "mpirun -np 1 ./HBTplus/build/HBT HBT_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f301e1",
   "metadata": {},
   "source": [
    "#### Output of run\n",
    "\n",
    "For each snapshot each HBT rank outputs two files.\n",
    "- The `SubSnap_xxx.y.hdf5` files contain information about the bound subhalos identified by HBTplus, including their particles. There are only a very small number of halo properties contained in these files. All information for merger trees is also contained within these files.\n",
    "- The `SrcSnap_xxx.y.hdf5` files contain a list of the particles associated with each subhalo, but which are not bound at the current snapshot. These files are only used if HBT if restarted, and so can be deleted once halo finding has been completed.\n",
    "\n",
    "HBT contains \"orphan\" subhalos. These are subhalos which have been disrupted, but are still tracked by the most bound particle at the time the last snapshot the subhalo was resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0976f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"Contents of HBT_output:\"\n",
    "ls HBT_output\n",
    "echo \"Output for snapshot 18:\"\n",
    "ls HBT_output/018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4393cc36",
   "metadata": {},
   "source": [
    "# SOAP - Calculating halo properties\n",
    "\n",
    "Dependencies\n",
    "- mpi4py\n",
    "- h5py built with parallel HDF5\n",
    "- Standard python modules (see requirements.txt)\n",
    "\n",
    "#### Clone repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e39466",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/SWIFTSIM/SOAP.git\n",
    "cd SOAP\n",
    "pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6e33df-3877-4a61-82c7-34d7ebd41154",
   "metadata": {},
   "source": [
    "#### Parameter file\n",
    "\n",
    "Set input and output directories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat > SOAP_config.yml << EOF\n",
    "#### Parameter file\n",
    "\n",
    "# Values in this section are substituted into the other sections\n",
    "# The simulation name (box size and resolution) and snapshot will be appended\n",
    "# to these to get the full name of the input/output files/directories\n",
    "Parameters:\n",
    "  sim_dir: $(pwd)\n",
    "  output_dir: $(pwd)\n",
    "  scratch_dir: $(pwd)\n",
    "\n",
    "# Location of the Swift snapshots:\n",
    "Snapshots:\n",
    "  # Use {snap_nr:04d} for the snapshot number and {file_nr} for the file number.\n",
    "  filename: \"{sim_dir}/snap_{snap_nr:04d}.hdf5\"\n",
    "\n",
    "# Which halo finder we're using, and base name for halo finder output files\n",
    "HaloFinder:\n",
    "  type: HBTplus\n",
    "  filename: \"{sim_dir}/HBT_output/{snap_nr:03d}/SubSnap_{snap_nr:03d}\"\n",
    "  fof_filename: \"{sim_dir}/fof_output_{snap_nr:04d}.hdf5\"\n",
    "\n",
    "GroupMembership:\n",
    "  # Where to write the group membership files\n",
    "  filename: \"{output_dir}/SOAP_uncompressed/membership_{snap_nr:04d}.hdf5\"\n",
    "\n",
    "HaloProperties:\n",
    "  # Where to write the halo properties file\n",
    "  filename: \"{output_dir}/SOAP_uncompressed/halo_properties_{snap_nr:04d}.hdf5\"\n",
    "  # Where to write temporary chunk output\n",
    "  chunk_dir: \"{scratch_dir}/SOAP-tmp/{halo_finder}/\"\n",
    "\n",
    "ApertureProperties:\n",
    "  properties:\n",
    "    {}\n",
    "  variations:\n",
    "    {}\n",
    "ProjectedApertureProperties:\n",
    "  properties:\n",
    "    {}\n",
    "  variations:\n",
    "    {}\n",
    "SOProperties:\n",
    "  properties:\n",
    "    CentreOfMass: true\n",
    "    CentreOfMassVelocity: true\n",
    "    Concentration: true\n",
    "    MassFractionSatellites: true\n",
    "    MassFractionExternal: true\n",
    "    NumberOfDarkMatterParticles: true\n",
    "    SORadius: true\n",
    "    SpinParameter: true\n",
    "    TotalMass: true\n",
    "  variations:\n",
    "    200_crit:\n",
    "      type: crit\n",
    "      value: 200.0\n",
    "    200_mean:\n",
    "      type: mean\n",
    "      value: 200.0\n",
    "    500_crit:\n",
    "      type: crit\n",
    "      value: 500.0\n",
    "SubhaloProperties:\n",
    "  properties:\n",
    "    CentreOfMass: true\n",
    "    CentreOfMassVelocity: true\n",
    "    NumberOfDarkMatterParticles: true\n",
    "    MaximumCircularVelocity: true\n",
    "    MaximumCircularVelocityUnsoftened: true\n",
    "    MaximumCircularVelocityRadiusUnsoftened: true\n",
    "    SpinParameter: true\n",
    "    TotalMass: true\n",
    "  variations:\n",
    "    Bound:\n",
    "      bound_only: true\n",
    "filters:\n",
    "  general:\n",
    "    limit: 100\n",
    "    properties:\n",
    "      - BoundSubhalo/NumberOfGasParticles\n",
    "      - BoundSubhalo/NumberOfDarkMatterParticles\n",
    "      - BoundSubhalo/NumberOfStarParticles\n",
    "      - BoundSubhalo/NumberOfBlackHoleParticles\n",
    "    combine_properties: sum\n",
    "  baryon:\n",
    "    limit: 100\n",
    "    properties:\n",
    "      - BoundSubhalo/NumberOfGasParticles\n",
    "      - BoundSubhalo/NumberOfStarParticles\n",
    "    combine_properties: sum\n",
    "  dm:\n",
    "    limit: 100\n",
    "    properties:\n",
    "      - BoundSubhalo/NumberOfDarkMatterParticles\n",
    "  gas:\n",
    "    limit: 100\n",
    "    properties:\n",
    "      - BoundSubhalo/NumberOfGasParticles\n",
    "  star:\n",
    "    limit: 100\n",
    "    properties:\n",
    "      - BoundSubhalo/NumberOfStarParticles\n",
    "calculations:\n",
    "  recalculate_xrays: false\n",
    "  calculate_missing_properties: false\n",
    "  min_read_radius_cmpc: 5\n",
    "  recently_heated_gas_filter:\n",
    "    delta_time_myr: 15\n",
    "    use_AGN_delta_T: true\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4964637-dc6f-43fc-8f97-0844e1e53c73",
   "metadata": {},
   "source": [
    "#### Running the group membership files\n",
    "\n",
    "Should be relatively cheap, just reads in the particles for each halo, resorts them, and outputs them in a SWIFT friendly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0f649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpirun -np 4 python3 -u SOAP/SOAP/group_membership.py \\\n",
    "    --sim-name=DM_test \\\n",
    "    --snap-nr=18 \\\n",
    "    SOAP_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f87c78-d5a2-4109-8544-008ed05a36e0",
   "metadata": {},
   "source": [
    "#### Calculating halo properties\n",
    "\n",
    "Chunks sets how to split up the simulation volume. We require multiple chunks if running on multiple nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d25f1-f71c-417f-9ef0-e361ca2d474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpirun -np 4 python3 -u SOAP/SOAP/compute_halo_properties.py \\\n",
    "    --sim-name=DM_test \\\n",
    "    --snap-nr=18 \\\n",
    "    --chunks=1 \\\n",
    "    --dmo \\\n",
    "    SOAP_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac031ea-7a37-463f-afc2-57987b3b595e",
   "metadata": {},
   "source": [
    "#### SOAP output\n",
    "\n",
    "Membership files have the same structure as a SWIFT snapshot. There are no particle IDs, but the particles are in the same order as the original snapshot.\n",
    "\n",
    "SOAP catalogues have a group for each halo type. All arrays have the same length (the number of subhalos), and are always in the same order.\n",
    "\n",
    "See Rob's workshop on Friday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5cf12e-5fd6-46a7-99a6-d33419da94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls SOAP_uncompressed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74915592-18f0-4183-af1a-2d599a837db6",
   "metadata": {},
   "source": [
    "#### Compression\n",
    "\n",
    "The files output from SOAP can be heavily compressed. For the membership files this is because most particles are not in a halo, and there are many repeated indices for the ones that are. For the halo properties we do not compute certain properties depending on the size of the input halo, so many values are zero.\n",
    "\n",
    "The membership files have no lossy compression filters, and can be compressed with h5repack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8068efc5-3ea8-4334-8984-5ad20d24b896",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5repack -i SOAP_uncompressed/membership_0018.hdf5 -o membership_0018.hdf5 -f GZIP=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d9f1a-a28b-47ff-a6e4-4966d5c08a0e",
   "metadata": {},
   "source": [
    "The halo catalogues have the same lossy compression filters as are available in SWIFT (some of which are custom), and so must be compressed using the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ad4f6-1a42-4ef7-8739-eec259f53c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "python SOAP/compression/compress_fast_metadata.py \\\n",
    "    SOAP_uncompressed/halo_properties_0018.hdf5 \\\n",
    "    halo_properties_0018.hdf5 \\\n",
    "    SOAP_uncompressed/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39a407-b471-462d-b77d-6d61f7d6217c",
   "metadata": {},
   "source": [
    "#### Generate documentation\n",
    "\n",
    "Documentation can be generated by running the `property_table.py`. You must pass the parameter file (to determine which halo types and properties are included in the documentation) and a SWIFT snapshot (to extract the units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d5583-a4f2-486c-a382-644113be9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd SOAP\n",
    "$python property_table.py ../SOAP_config.yml ../snap_0018.hdf5\n",
    "cd documentation\n",
    "# pdflatex SOAP.tex\n",
    "cd ../.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
